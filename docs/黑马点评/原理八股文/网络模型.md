---
title: Redis网络模型
date: 2026-1-28
---

### 用户空间与内核空间

在操作系统中，为了防止用户进程冲突导致系统崩溃，将用户与内核的空间分开：

- 用户进程只能够调用用户空间，但可以通过系统提供的系统调用来调用内核资源
- 内核空间可以执行特权指令调用系统资源

比如linux在写时，会将用户缓存数据拷贝到内核缓冲区，然后写入设备

读时，从设备获取数据到内核缓冲区，然后拷贝到用户缓冲区



### 阻塞IO（BIO）

即第一阶段必须阻塞进程执行下一步操作：

以linux的读写为例，用户进程执行系统调用后，必须阻塞等待结果：等待数据写入内核缓冲区，等待数据拷贝至用户缓冲区。



### 非阻塞IO（NIO）

即第一阶段必定不阻塞进程执行下一步操作（直接返回结果）：

用户进程尝试获取数据，若内核缓冲区没有数据，则直接返回失败，只有数据已存在内核缓冲区才会直接返回结果



### IO多路复用

> 减少IO带来的时间开销，尽可能的使Redis的单线程高效，全时间段的执行业务
> 注意是阻塞等待的，并非NIO

在单线程下，我们对于请求的处理只能是以队列的方式一个一个来，这样效率非常低下，尤其是**轮到的socket连接尚未完成IO操作**，如果不采用抢占式的处理，就会导致单线程被阻塞，其他请求必须等待。

针对这样的问题，我们可以**准备多IO通道同时监听请求是否已经完成IO等以确保就绪**并优先处理这些请求来避免一个连接阻塞全部连接，那么我们要怎么实现对socket的监听呢？

在linux中，socket会被记录为一个文件，我们就可以通过监听他的**文件描述符（FD）**来完成就绪状态的监听，准备一个单线程专门用于监听多个FD，当某个FD表示为已就绪时就可以得到通知。

而这个单线程监听通知，其实也有多种方案：

#### select

**🔹 核心机制：**

- 使用 **位图（bitmask）** 表示 fd 集合（`fd_set`，通常 1024 位 → 最多监听 1024 个 fd）
- 每次调用需 **传入完整 fd 集合**
- 内核**遍历**所有 fd，检查是否就绪
- 返回时，**修改原 fd_set**，仅保留就绪的 fd

 **⚠️ 缺点：**

| 问题               | 说明                                   |
| :----------------- | :------------------------------------- |
| **fd 数量限制**    | 默认 `FD_SETSIZE = 1024`，不可动态扩展 |
| **每次拷贝开销大** | 用户态 ↔ 内核态 **全量拷贝 fd_set**    |
| **线性扫描 O(n)**  | 内核需遍历所有 fd（即使只有 1 个活跃） |
| **不可重用**       | 每次调用必须重新设置 fd_set            |

 **✅ 优点：**

- 跨平台（Windows / Unix / Linux 都支持）
- 简单易用（早期网络程序常用）



#### poll

**🔹 改进点：**

- 使用 **数组 of `pollfd`** 替代位图 → **突破 1024 限制**
- 分离“关注事件”和“返回事件” → **无需重置结构**

**⚠️ 仍存在的问题：**

| 问题           | 说明                                                 |
| :------------- | :--------------------------------------------------- |
| **全量拷贝**   | 每次调用仍需将整个 `pollfd` 数组从用户态拷贝到内核态 |
| **O(n) 扫描**  | 内核仍需线性遍历所有 fd                              |
| **无就绪缓存** | 每次调用都要重新检查所有 fd                          |

**✅ 优点：**

- 无 fd 数量硬限制（受限于内存）
- 接口更清晰（事件分离）



#### epoll

| 特性                 | 说明                                                         |
| :------------------- | :----------------------------------------------------------- |
| **1. 内核事件表**    | 通过 `epoll_ctl` **预先注册 fd**，内核维护**红黑树 + 就绪链表** |
| **2. 零拷贝通知**    | `epoll_wait` **只返回就绪的 fd**，无需全量传递               |
| **3. O(1) 事件检测** | 基于回调机制：socket 就绪时，**主动加入就绪链表**            |

**📌 工作流程：**

1. `epoll_create()` → 创建内核事件表
2. `epoll_ctl(ADD, fd, event)` → 将 fd 加入红黑树，并注册回调
3. 当 fd 就绪 → 内核自动将其加入 **就绪队列**
4. `epoll_wait()` → 直接返回就绪队列中的 fd（**无需遍历全部**）

**✅ 优势总结：**

| 对比项         | `select`/`poll` | `epoll`              |
| :------------- | :-------------- | :------------------- |
| **fd 数量**    | 有限 / OOM 风险 | 百万级（受限于内存） |
| **时间复杂度** | O(n) 每次调用   | O(1) 事件通知        |
| **数据拷贝**   | 每次全量拷贝    | 仅拷贝就绪 fd        |
| **适用场景**   | 小规模连接      | **高并发（C10K+）**  |

> 💡 **Redis、Nginx、Kafka 等均使用 `epoll` 实现网络模型**。

| 特性             | `select`                 | `poll`               | `epoll`                        |
| :--------------- | :----------------------- | :------------------- | :----------------------------- |
| **跨平台**       | ✅ 是                     | ✅ 是                 | ❌ Linux 专属                   |
| **最大 fd 数**   | 1024（默认）             | 无硬限制             | 无硬限制（`ulimit -n`）        |
| **fd 存储结构**  | 位图（`fd_set`）         | 数组（`pollfd`）     | 内核红黑树 + 就绪链表          |
| **每次调用开销** | 全量拷贝 + O(n) 扫描     | 全量拷贝 + O(n) 扫描 | **仅返回就绪 fd**              |
| **事件通知效率** | 低（活跃 fd 少时浪费大） | 中                   | **极高（O(1)）**               |
| **支持 ET 模式** | ❌ 否                     | ❌ 否                 | ✅ 是（`EPOLLET`）              |
| **典型应用**     | 老旧系统、教学           | 中等规模服务         | **Redis / Nginx / 高并发服务** |



### IO多路复用的事件通知机制（LT，ET）

**LT（Level-Triggered，水平触发）** 与 **ET（Edge-Triggered，边缘触发）** 是 I/O 多路复用中两种**事件通知的语义模型**，它们决定了操作系统在什么条件下向应用程序“报告”一个文件描述符（如 socket）已就绪。虽然 `select` 和 `poll` 只支持 LT，但 **`epoll` 同时支持 LT 和 ET**，这也是 `epoll` 更强大、更灵活的关键原因之一。

| 模型               | 触发条件                                       | 类比                                           |
| :----------------- | :--------------------------------------------- | :--------------------------------------------- |
| **LT（水平触发）** | **只要 fd 处于“就绪状态”，就持续通知**         | 就像水位传感器：只要水位高于警戒线，就一直报警 |
| **ET（边缘触发）** | **仅当 fd 状态“从非就绪变为就绪”时，通知一次** | 就像门铃：只在你按下的瞬间响一次，之后不再响   |

> 🔑 **关键区别：是否“重复通知”未处理完的就绪事件。**

**触发条件：**

| 事件类型                 | LT 触发条件               | ET 触发条件                      |
| :----------------------- | :------------------------ | :------------------------------- |
| **可读（EPOLLIN）**      | 接收缓冲区 **有数据**     | 接收缓冲区 **从空 → 非空**       |
| **可写（EPOLLOUT）**     | 发送缓冲区 **有空闲空间** | 发送缓冲区 **从满 → 有空闲**     |
| **对端关闭（EPOLLHUP）** | 对端关闭连接              | 对端关闭连接（通常也只触发一次） |

> 💡 注意：**ET 模式下，如果没处理完事件，可能永远丢失数据！**

**对比：**

| 维度             | LT                        | ET                                |
| :--------------- | :------------------------ | :-------------------------------- |
| **系统调用次数** | 多（重复通知）            | 少（仅状态变化时通知）            |
| **CPU 开销**     | 略高（尤其活跃连接少时）  | 更低（适合高并发）                |
| **编程复杂度**   | 简单                      | 复杂（需处理不完整 I/O）          |
| **容错性**       | 高（不怕漏读）            | 低（漏读 = 数据丢失）             |
| **适用场景**     | 通用服务（如 Redis 默认） | 极致性能场景（如 DPDK、自研框架） |

> LT模式下，epoll_wait读取完成后，若仍有fd残留没有读完，那么就不会将任意一个连接fd从就绪队列中删除，而ET则会直接删除
>
> | 行为                 | LT 模式                 | ET 模式                      |
> | :------------------- | :---------------------- | :--------------------------- |
> | 收到 10 字节         | 标记为就绪              | **触发一次通知**（状态变化） |
> | 读 5 字节            | 仍就绪 → **下次还通知** | 仍就绪 → **下次不通知！**    |
> | 读完 10 字节         | 退出就绪                | 退出就绪                     |
> | 新数据到来（3 字节） | 再次就绪 → 通知         | **状态变化 → 再次通知**      |
>
> 但是**旧数据一旦被读走（从 socket 接收缓冲区中取出），就永远消失了，不可能再被读到。**
>
> | 阶段                       | LT 行为                    | ET 行为                       |
> | :------------------------- | :------------------------- | :---------------------------- |
> | **1. 收到 "HelloWorld"**   | 通知可读                   | 通知可读（状态变化）          |
> | **2. 你读完 "HelloWorld"** | 不再通知（缓冲区空）       | 不再通知                      |
> | **3. 收到 "NewMsg"**       | **再次通知**（缓冲区非空） | **再次通知**（状态从空→非空） |
> | **4. 你读 "NewMsg"**       | 只读到新数据               | 只读到新数据                  |

| 特性           | LT（水平触发）       | ET（边缘触发）              |
| :------------- | :------------------- | :-------------------------- |
| **通知频率**   | 持续通知（只要就绪） | 仅状态变化时通知一次        |
| **编程难度**   | 简单                 | 复杂（需非阻塞 + 循环 I/O） |
| **数据安全性** | 高（不怕漏处理）     | 低（漏处理 = 数据丢失）     |
| **性能潜力**   | 中                   | 高（特定场景）              |
| **是否默认**   | ✅ 是（`epoll` 默认） | ❌ 需显式开启                |
| **适用人群**   | 所有人               | 高级开发者                  |

> **Redis 在 LT 模式下（默认），对每个客户端连接：**
>
> 1. **尽可能一次性读完 socket 接收缓冲区中的所有可用数据**（通过非阻塞 I/O 循环读取）；
> 2. **将读到的数据放入客户端私有的 query buffer**；
> 3. **然后逐条解析并执行其中的命令**；
> 4. **不会重复执行命令，也不会因“旧数据残留”导致错误**。
>
> 通过`query buffer`，Redis是**不会执行记录过的命令的**，调用大致流程：
>
> 1. **调用 `readQueryFromClient(client)`**
> 2. 在该函数内部：
>    - 使用 **非阻塞 socket**
>    - **循环调用 `read()`**，直到返回 `EAGAIN`（表示暂时无更多数据）
>    - 将所有读到的数据 **追加到 `client->querybuf`（SDS 字符串）**
> 3. 然后调用 `processInputBuffer(client)`
>    - 从 `querybuf` 中 **按 Redis 协议（RESP）逐条解析命令**
>    - 每解析出一条完整命令，就立即执行（`call()`）
>    - 执行完后，**从 `querybuf` 中移除已处理的部分**



### IO多路复用的流程

```
[Client] 
    │
    ↓ (TCP 连接)
[Web Server]
├── 1. 创建监听 socket（listen_fd）
├── 2. epoll_create() → 创建事件中心
├── 3. epoll_ctl(ADD, listen_fd, EPOLLIN) → 监听连接事件
└── 4. 事件循环：while (1) { epoll_wait() → 处理事件 }
        ├── 若 listen_fd 就绪 → accept() 新连接
        └── 若 client_fd 就绪 → read() → parse → execute → write()
```

> **`listen_fd`（监听 socket）就绪 ≠ 客户端请求数据就绪，而是表示“有新连接到达，可以调用 `accept()` 了”。**
>
> **当有新连接产生时，`listen_fd`作为监听socket就会就绪，epoll接收到后就会尝试去通过accept获取连接信息建立连接（`listen_fd`本身不是存储连接信息的fd，单节点中全局唯一，就是个门铃）**
>
> **`accept()` 的作用是：从内核的“已完成连接队列”中取出一个新连接，并返回一个新的文件描述符（`client_fd`），用于后续与该客户端通信。**
>
> **你不能直接用 `listen_fd` 读写客户端数据！必须通过 `accept()` 拿到 `client_fd` 才能通信。**

当服务端调用 `listen(fd, backlog)` 后，内核会为该 socket 维护 **两个队列**：

| 队列                          | 作用                                               | 状态          |
| :---------------------------- | :------------------------------------------------- | :------------ |
| **SYN 队列（半连接队列）**    | 存放收到 SYN 但未完成三次握手的连接                | `SYN_RECV`    |
| **Accept 队列（全连接队列）** | 存放**已完成三次握手**的连接，等待 `accept()` 取走 | `ESTABLISHED` |

```
Client                    Server
  │                         │
  │-------- SYN ----------->│ → 放入 SYN 队列（内核控制，非epoll控制）
  │<------- SYN+ACK --------│
  │-------- ACK ----------->│ → 移入 Accept 队列
  │                         │
  │                         │ ← listen_fd 变为“可读”
  │                         │ ← 应用调用 accept()
  │<------ 新连接 fd --------│ → 从 Accept 队列取出，返回 client_fd
```

> 🔑 **只有当 Accept 队列非空时，`listen_fd` 才会变为“可读”（即 `epoll_wait` 返回它）**。

`listen_fd` 本质上是一个 **监听 socket**，它的唯一职责是：

> **“当有新 TCP 连接完成三次握手后，通知应用程序调用 `accept()` 来领取这个连接。”**

它**不存储任何具体客户端的信息**，只维护两个队列：

- **SYN 队列**（半连接）
- **Accept 队列**（已完成连接）

但这些队列中的**每个连接项**，是由内核单独管理的，与 `listen_fd` 本身是分离的。

总结一下，我们寻常使用Redis时，会使用**RedisClient**去先**连接Redis节点**，在连接时，Redis节点内核会**将`listen_fd`设置为就绪**，epoll_wait在读出这条fd后尝试通过**accept**()到**Accept队列**中获取连接信息，**得到`client_fd`**；

> **`client_fd` 是通信管道，请求数据通过它流入 Redis 的接收缓冲区，再由协议解析器（RESP）解析成具体操作。**

这个`client_fd`即与RedisClient的连接socket文件fd，当我们在**RedisClient**中向节点尝试**发送命令调用**时，**Redis节点中对应的`client_fd`就会获取到详细的操作信息并被内核设置为就绪**，等待epoll_wait调用**解析 RESP 协议**执行业务。

> Redis节点与客户端Client的连接会持续到一方下线为止，并不是Client执行一次命令，就tcp再建立一次连接



> **补充：**
>
> **I/O 多路复用是「用户空间程序」与「操作系统内核」之间的协作机制，用于高效监控多个文件描述符（fd）的 I/O 就绪状态。**
>
> ❌ **它不是客户端与服务端之间的通信机制！**
>
> 但是我们确实可以使用它来完成客户端与服务端之间的**连接的IO监控**，就像是以上作为通信管道的`client_fd`。
>
> 用户空间程序：redis-server
>
> 操作系统内核：fd文件



### 信号驱动IO

基本的非阻塞，系统调用后先返回结果，如无数据则内核检测数据，当数据生成后发送信号通知用户应用来获取数据（二阶段阻塞）

但是需要维护大量的表，可能栈溢出

内核与用户空间交互性能低



### 异步IO

完全的非阻塞，用户系统调用请求数据，内核等待数据并写到缓冲区，然后直接返回给用户

复杂度比较高，并发控制难度大

> 只有二阶段是非阻塞（用户不需要阻塞等待业务执行）



### Redis的网络模型

#### Redis到底算不算是单线程？

只论业务处理（nosql命令处理），是的。

谈论整个Redis，其实是多线程（几个开销比较大的任务，为了不阻塞主线程，会额外开一个线程完成这些任务）

6.0中对客户端命令解析以及写响应结果的任务使用多线程实现
