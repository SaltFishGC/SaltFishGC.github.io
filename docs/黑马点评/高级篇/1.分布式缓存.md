---
title: 分布式缓存
date: 2026-1-25
---

### Redis持久化

我们说数据持久化一般都是将数据存储到磁盘中，比方说数据库。而Redis也存在这么一种持久化，我们可以将内存中存储

#### RDB持久化

Redis会在**固定时间点**以及**手动关机**时进行**数据快照**，将内存中存储的数据**持久化**为**rdb快照文件**，将快照文件保存到硬盘中，当下次启动时会尝试从rdb文件中获取旧数据。

那么如何设置这个固定时间点呢？

我们可以在**conf文件内**对相关设置做配置：

**save设置rdb时间点：**

```bash
save <seconds> <changes>
```

- 表示 **在 `<seconds>` 秒内，如果有至少 `<changes>` 次写操作**，就触发一次 `bgsave`（后台保存 RDB 快照）。
- 可以配置多条 `save` 规则，满足任意一条即触发快照。

> 假如不希望开启RDB，使用`save ""`显式禁用RDB或是注释掉所有save行

**默认配置（Redis 官方默认值）：**

```bash
save 900 1      # 15分钟内至少有1次修改 → 触发快照
save 300 10     # 5分钟内至少有10次修改 → 触发快照
save 60 10000   # 1分钟内至少有10000次修改 → 触发快照
```

> 💡 这些规则是“**或**”的关系，只要满足其中任意一条，就会执行 `bgsave`。

| 配置项                            | 说明                                                         |
| :-------------------------------- | :----------------------------------------------------------- |
| `dbfilename dump.rdb`             | RDB 文件名，默认为 `dump.rdb`                                |
| `dir ./`                          | RDB 文件保存的目录（必须是可写目录）                         |
| `stop-writes-on-bgsave-error yes` | 如果 `bgsave` 失败，是否禁止写入（防止数据不一致）           |
| `rdbcompression yes`              | 是否对 RDB 文件启用 LZF 压缩（节省空间，默认开启）           |
| `rdbchecksum yes`                 | 是否在 RDB 文件末尾添加 CRC64 校验（提高数据完整性，默认开启） |

> 但是假如发生了宕机/断电等突发情况，单机模式下，数据是无法生成rdb快照文件的，那么这时候数据就会丢失！
>



**那么bgsave是怎么实现的呢？**

我们说Redis是单线程服务，但是如果让主线程阻塞先完成数据的持久化好像不太行，如果在持久化时出现请求要求读写Redis数据就存在问题了，那么这时候我们就需要**一个额外的线程**来执行持久化操作了。

我们fork主线程得到一个子线程，子线程共享主线程的内存资源，就可以读取Redis在内存中的数据从而完成读写操作了。

但是我们知道为了保证资源安全，线程之间共享的资源是会上**ReadOnly**锁的，那么这时候假如有写入/修改需求怎么办？

事实上在持久化时，只读部分数据的需要修改部分会被创造一个副本用于被主线程执行读写操作，完成后覆盖。注意**Redis 使用操作系统的「写时复制」（Copy-On-Write, COW）机制，只在需要时复制被修改的内存页，而不是一开始就复制全部数据。**

> 这里子线程写入RDB文件的是只读部分的旧数据，而非新写入的数据。



对于RDB，在两次写入之间如果发生宕机就会丢失数据。



#### AOF持久化

AOF（**Append Only File**）是 Redis 提供的一种**持久化机制**，通过记录服务器接收到的**每一个写操作命令**，以日志的形式追加到文件末尾。在 Redis 重启时，通过**重放（replay）这些命令**来恢复数据。

**对AOF的配置：**

```bash
appendonly yes # 开启AOF持久化
appendfilename "appendonly.aof" # AOF文件.
appendfsync everysec # 同步方案，默认everysec
```

| 配置值     | 行为                                 | 数据安全性         | 性能                           | 适用场景                             |
| :--------- | :----------------------------------- | :----------------- | :----------------------------- | :----------------------------------- |
| `always`   | 每个写命令都 `fsync` 到磁盘          | ⭐⭐⭐ 几乎不丢数据   | ❌ 最差（每次写都要等磁盘 I/O） | 对数据零丢失要求极高的场景（极少用） |
| `everysec` | 每秒 `fsync` 一次（**默认**）        | ⭐⭐ 最多丢 1 秒数据 | ✅ 平衡                         | **绝大多数生产环境推荐**             |
| `no`       | 由操作系统决定何时写盘（通常 30 秒） | ⭐ 可能丢几十秒数据 | ⚡ 最好                         | 不关心数据丢失的缓存场景，不如RDB    |



**AOF 的核心原理**

1. **每当执行一个写命令**（如 `SET`, `HSET`, `LPUSH` 等），Redis 就将该命令以 **RESP 协议格式**追加到 AOF 缓冲区。
2. 根据配置的 **同步策略（appendfsync）**，将缓冲区内容写入并同步到磁盘上的 AOF 文件。
3. **Redis 重启时**，会读取 AOF 文件，逐条执行其中的命令，重建原始数据集。

> 📌 AOF 文件是**可读的文本文件**，你可以用 `cat` 查看，甚至手动修复（但需谨慎）。

示例 AOF 内容：

```
*2
 $ 6
SELECT
 $ 1
0
*3
 $ 3
SET
 $ 3
foo
 $ 3
bar
```



**AOF 的问题：文件会无限增长！**

- 每次 `INCR counter` 都会记录一条命令，即使最终值只是 +1。
- 删除 key 后，AOF 仍保留创建和删除的全部历史。
- 长期运行后，AOF 文件可能变得**巨大且冗余**。

**解决方案：AOF 重写（Rewrite）**

**原理：**

- **不是重放旧命令**，而是**基于当前数据库状态，生成最简命令集**。
- 例如：100 次 `INCR` → 直接 `SET counter 100`
- 删除的 key 不会被写入新 AOF 文件。

**触发方式：**

- 手动：`BGREWRITEAOF` 命令

- 自动（配置）：

  ```bash
  auto-aof-rewrite-percentage 100 # 比上次文件增长超过100%发生重写
  auto-aof-rewrite-min-size 64mb # AOF文件最小多大才会触发重写
  ```

  表示：当 AOF 文件比上次重写后增长超过 100% 且大于 64MB时，自动触发重写。


**重写过程（关键！）：**

1. 主进程 `fork()` 出子进程。
2. 子进程根据当前内存数据**生成新的 AOF 文件**（使用 COW，不影响主进程）。
3. 主进程继续处理请求，并将**新写命令同时写入旧 AOF 和 AOF 重写缓冲区**。
4. 子进程完成新 AOF 后，主进程将**缓冲区中的增量命令追加到新文件**。
5. 原子替换旧 AOF 文件。

> ✅ 整个过程**不阻塞主线程**，保证服务可用性。



#### RDB以及AOF的比较

| 对比维度         | RDB                      | AOF                    |
| :--------------- | :----------------------- | :--------------------- |
| **持久化方式**   | 定时快照**数据**         | 实时**命令**日志       |
| **数据完整性**   | 较低（可能丢分钟级数据） | 高（最多丢 1 秒）      |
| **恢复速度**     | 快                       | 慢                     |
| **文件大小**     | 小                       | 大（可重写优化）       |
| **对性能影响**   | 低（后台 bgsave）        | 中（每秒 fsync）       |
| **可读性**       | 不可读（二进制）         | 可读（文本）           |
| **适合用途**     | 备份、**快速**恢复、缓存 | 高**安全**要求、主存储 |
| **是否默认开启** | 是                       | 否                     |

> 高要求的复杂架构下**不要二选一，而是“组合使用”！**

```bash
# 同时开启 RDB 和 AOF
save 900 1
save 300 10
appendonly yes
appendfsync everysec
aof-use-rdb-preamble yes
```

- **RDB** 用于：定期备份、快速冷启动、灾难恢复。
- **AOF** 用于：保证运行时数据不丢失。
- **混合模式** 让你鱼与熊掌兼得。







### 主从集群

Redis 的**主从集群（Master-Slave Replication）** 是 Redis 提供的一种**高可用、读写分离和数据冗余**的基础架构。它通过将一个 Redis 实例（主节点）的数据**异步复制**到一个或多个从节点（Slave），实现故障转移、负载分担和数据备份。

| 目标           | 说明                                                         |
| :------------- | :----------------------------------------------------------- |
| **数据冗余**   | 主节点宕机后，从节点保留副本，避免数据丢失                   |
| **读写分离**   | **写**操作走**主节点**，**读**操作可分发到多个**从节点**，提升吞吐 |
| **高可用基础** | 为哨兵（Sentinel）或 Redis Cluster 提供故障转移能力          |
| **灾备与备份** | 从节点可作为冷备，用于数据恢复或分析                         |

在conf中完成配置：

**主节点通常不需要额外配置：**

```bash
# redis-master.conf（可选，通常用默认配置即可）
port 6379
bind 0.0.0.0
protected-mode no          # 生产环境建议用密码+防火墙替代
daemonize yes
pidfile /var/run/redis_6379.pid
logfile /var/log/redis/redis-server.log
dir /var/lib/redis

# 主节点默认就是可写的，无需 replicaof 配置
# 不要写 replicaof 或 slaveof！
```

> 🔔 **主节点不能包含 `replicaof` 或 `slaveof` 指令**，否则它会变成从节点！

**从节点：**

关键在于指定主节点：`replicaof 192.168.1.100 6379`

```bash
# 端口（避免与主节点冲突）
port 6380

# 绑定地址
bind 0.0.0.0

# 后台运行
daemonize yes

# 日志和 PID
logfile /var/log/redis/redis-replica.log
pidfile /var/run/redis_6380.pid

# 数据目录
dir /var/lib/redis

# 关键：指定主节点 IP 和端口
replicaof 192.168.1.100 6379

# 只读模式（默认开启，推荐保持）
replica-read-only yes

# 如果主节点有密码，需配置（重要！）
masterauth your_master_password

# 如果使用 ACL（Redis 6+），可能需要 replicauser（较少用）
# replica-announce-ip 192.168.1.101   # 用于 NAT/代理场景（可选）
# replica-announce-port 6380
```

> 📌 **注意**：
>
> - Redis 5.0 起，`slaveof` 已废弃，改用 `replicaof`。
> - 如果主节点设置了密码（`requirepass`），**必须配置 `masterauth`**，否则复制失败。







### 主从节点的数据同步

#### 全量同步

Redis 的**全量同步（Full Synchronization）** 是主从复制（Replication）过程中，**从节点（Replica）首次连接主节点（Master）或断连太久无法进行部分同步时，完整拷贝主节点当前全部数据的过程**。它是 Redis 数据冗余和高可用架构的基础环节。

**发生全量同步的场景：**

| 场景                                                         | 说明                                 |
| :----------------------------------------------------------- | :----------------------------------- |
| **1. 从节点初次启动**                                        | 第一次连接主节点，没有历史数据       |
| **2. 从节点 runid 与主节点记录的不一致**                     | 比如从节点重启后 runid 改变          |
| **3. 从节点请求的复制偏移量（offset）已不在主节点的复制积压缓冲区（Replication Backlog）中** | 网络断开时间过长，主节点已覆盖旧命令 |
| **4. 执行 `REPLICAOF` 命令切换主节点**                       | 重新建立复制关系                     |

> 🔍 可通过 `INFO REPLICATION` 查看 `master_sync_in_progress:1` 判断是否正在全量同步。



全量同步**流程**：

**步骤 1：从节点发起同步请求**

```bash
# 从节点发送（初次同步）
PSYNC ? -1
```

- `?` 表示不知道主节点的 runid
- `-1` 表示没有有效偏移量

------

**步骤 2：主节点响应并准备 RDB 快照**

- 主节点生成一个**唯一的 runid**（如果尚未生成）
- 主节点执行 `bgsave`，**fork 子进程生成 RDB 文件**
- 同时，主节点开启一个**复制客户端缓冲区（replication buffer）**，缓存 `bgsave` 期间的所有写命令

> ⚠️ 此阶段主节点仍可正常处理读写请求（非阻塞），但 fork 和磁盘 I/O 会有性能影响。

------

**步骤 3：主节点发送 RDB 文件给从节点**

- RDB 文件通过 socket **直接传输**给从节点（支持无盘复制 `repl-diskless-sync`，但默认走磁盘）
- 从节点**清空自身现有数据**（如果有）

------

**步骤 4：从节点加载 RDB 文件**

- 从节点将接收到的 RDB 文件**加载到内存**，重建数据库状态
- 此过程会**阻塞从节点的主线程**，期间无法响应客户端请求（但不影响主节点）

------

**步骤 5：主节点发送缓存的增量命令**

- RDB 传输完成后，主节点将**复制缓冲区中积累的写命令**发送给从节点
- 从节点**依次执行这些命令**，使数据状态与主节点完全一致

------

**步骤 6：进入稳定复制阶段**

- 全量同步完成，主从进入**命令传播（Command Propagation）** 阶段
- 主节点每执行一个写命令，就立即发送给从节点（异步）



**如何判断需要什么同步呢？**其实可以从slave节点传输的两个参数来判断：

一个是**Replication Id/runid**：数据集标记，id一致则说明是同一数据集，每一个主节点都有自己的id，从节点会继承主节点的id；

另一个是**offset**：偏移量，随着业务的运行，log文件会逐渐增长，需要记录最新的长度，主从节点的offset不一致，则需要进行同步；

那么主节点在收到同步请求的时候，就会对以上参数做判断：

**步骤 1：检查 runid 是否匹配**

- 如果从节点发的是 `?` → **runid 不匹配** → **必须全量同步**

- 如果从节点发的是具体 runid：

  - 与当前主节点的 `runid`**不一致** → 全量同步

    > （比如主节点重启过，runid 已变）

  - **一致** → 进入下一步

**步骤 2：检查 offset 是否在 backlog 范围内**

- 主节点维护一个**复制积压缓冲区（Replication Backlog）**，记录最近的写命令及其偏移量范围 `[start_offset, end_offset]`
- 如果从节点请求的 `offset` **≥ start_offset** → **可以部分同步**
- 如果 `offset < start_offset`（已被覆盖）→ **必须全量同步**

| 从节点发送的 PSYNC               | 主节点判断逻辑            | 结果                         |
| :------------------------------- | :------------------------ | :--------------------------- |
| `PSYNC ? -1`                     | runid 未知                | **全量同步**（典型初次同步） |
| `PSYNC <旧runid> <任意offset>`   | runid ≠ 当前主节点 runid  | **全量同步**（主节点已重启） |
| `PSYNC <当前runid> <有效offset>` | offset 在 backlog 中      | **部分同步**                 |
| `PSYNC <当前runid> <过期offset>` | offset < backlog 起始位置 | **全量同步**                 |



#### 增量同步

如果都做全量同步，那么就会需要额外的资源来完成数据的同步，在一些情况下，我们其实可以只对主节点产生的新数据/命令进行加载而不需要完全清空从节点的数据，那么这时候我们就可以使用增量同步（部分同步）。

**当且仅当同时满足以下两个条件时，Redis 会使用增量同步：**

**条件 1：从节点提供的 `runid` 与当前主节点的 `runid` 一致**

- 表示从节点之前确实是从**当前这个主节点**复制数据的。
- 如果主节点重启过（`runid` 改变），或从节点切换了主节点，则不满足。

**条件 2：从节点请求的复制偏移量（offset）仍在主节点的「复制积压缓冲区」（Replication Backlog）范围内**

- 主节点维护一个固定大小的环形缓冲区（默认 1MB），缓存最近的写命令及其偏移量。
- 如果从节点断连期间，主节点写入的数据量 **没有超过 backlog 大小**，则缺失的数据还在缓冲区中。

> ✅ 只有这两个条件都满足，主节点才会回复 `+CONTINUE`，并只发送缺失的命令（增量同步）。

> 简单记忆：
>
> 一、主节点id必须相同
>
> 二、丢失的更新记录log不能过多（主节点只记录一个环形缓冲区，不在其中则无法追溯）

两者的比较：

| 特性           | 增量同步（Partial Sync）    | 全量同步（Full Sync）          |
| :------------- | :-------------------------- | :----------------------------- |
| **触发条件**   | runid 匹配 + offset 有效    | 否则                           |
| **数据传输量** | 仅缺失的写命令（KB～MB 级） | 全量 RDB（GB 级）              |
| **主节点开销** | 极低（仅读 backlog）        | 高（fork + bgsave + 网络 I/O） |
| **从节点开销** | 低（执行少量命令）          | 高（加载 RDB 阻塞主线程）      |
| **恢复时间**   | 毫秒～秒级                  | 秒～分钟级（取决于数据量）     |



#### 对同步的优化

| 优化方向                            | 配置项 / 措施                                         | 原理说明                                                     | 优点                                                         | 缺点 / 注意事项                                              |
| ----------------------------------- | ----------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1. 无磁盘复制（Diskless Sync）      | `repl-diskless-sync yes` `repl-diskless-sync-delay 5` | 主节点不生成 RDB 文件到磁盘，而是通过 socket 直接将 RDB 数据流发送给从节点，避免磁盘 I/O。延迟可调（单位：秒），等待更多客户端连接后批量发送。 | - 减少主节点磁盘压力 - 提升全量同步效率（尤其在高 I/O 负载时） | - 网络要求高（需稳定、低延迟） - 若网络中断，可能需要重传 - 不适合大文件或弱网环境 |
| 2. 控制单节点内存大小               | 限制单个 Redis 实例内存占用（如 < 8GB）               | 内存越小，`bgsave` fork 和 RDB 生成越快，减少对主节点性能影响。 | - 减少 fork 时间和内存开销 - 降低 RDB 持久化带来的磁盘 I/O 压力 | - 单机容量有限，不适合超大规模数据 - 需结合分片（Cluster）扩展 |
| 3. 增大复制积压缓冲区（Backlog）    | `repl-backlog-size 256mb` 或更大                      | Backlog 是主节点维护的环形缓冲区，用于存储最近的写命令。增大其大小可以支持更长时间的断连恢复，提高部分同步成功率。 | - 减少全量同步频率 - 快速恢复网络闪断后的数据一致性          | - 占用主节点内存 - 不宜过大（建议根据写入量估算）            |
| 4. 限制从节点数量                   | 控制每个 master 下的 replica 数量（建议 ≤ 5）         | 多个从节点同时请求同步时，主节点需处理多个连接和数据传输，增加 CPU 和网络负载。 | - 减轻主节点压力 - 提高复制稳定性                            | - 可能导致读能力不足 - 可采用“主-从-从”链式结构缓解          |
| 5. 采用主-从-从链式结构（级联复制） | 设置一个 slave 作为其他 slave 的 master（A → B → C）  | 将部分从节点连接到上游从节点，而不是直接连接主节点，减轻主节点的复制压力。 | - 分摊主节点网络带宽 - 支持多层级复制，适合跨机房部署        | - 增加复制延迟（级联层越多，延迟越高） - 不适合强一致性场景  |
| 6. 优化主节点配置                   | `tcp-keepalive 300` `timeout 60`                      | 防止因网络空闲导致连接断开，减少不必要的全量同步。           | - 保持长连接 - 提高复制稳定性                                | - 过长可能导致资源浪费                                       |
| 7. 合理设置复制缓冲区上限           | `client-output-buffer-limit replica 256mb 64mb 60`    | 限制从节点输出缓冲区大小，防止 OOM。当缓冲区超过阈值时，主节点会关闭连接。 | - 防止从节点卡住导致主节点阻塞 - 保护主节点稳定性            | - 设置过小可能频繁断连 - 需根据网络状况调整                  |
| 8. 使用混合持久化（Redis 4.0+）     | `aof-use-rdb-preamble yes`                            | AOF 重写时使用 RDB 快照 + AOF 增量命令，重启时先加载 RDB（快），再执行 AOF 命令（准）。 | - 加快从节点启动速度 - 提高数据安全性                        | - 仅适用于 AOF 开启的场景                                    |







### 哨兵机制

Redis **哨兵（Sentinel）** 是 Redis 官方提供的**高可用（High Availability, HA）解决方案**，用于**监控、自动故障检测和主从切换**。当主节点（Master）宕机时，Sentinel 能自动将一个从节点（Replica）提升为新的主节点，并通知客户端更新连接地址，从而实现**无人工干预的故障恢复**。

> 📌 哨兵不负责数据存储或分片，只负责**管理主从集群的拓扑和状态**。

#### 功能及实现

| 功能                                        | 说明                                                         |
| :------------------------------------------ | :----------------------------------------------------------- |
| **1. 监控（Monitoring）**                   | 持续检查主节点、从节点是否正常运行                           |
| **2. 通知（Notification）**                 | 当节点故障时，可通过 API 或脚本通知管理员                    |
| **3. 自动故障转移（Automatic Failover）**   | 主节点宕机后，自动选举一个从节点升级为主，并重新配置其他从节点复制新主节点 |
| **4. 配置提供者（Configuration Provider）** | 客户端（如 Lettuce）通过 Sentinel 获取当前主节点地址         |

> - 为什么至少 3 个从节点？
>
>   故障判定需要多数派投票（quorum），避免“脑裂”（Split-Brain）。
>
>   - 1 个 Sentinel：无法判断是主挂了还是自己网络问题
>   - 2 个 Sentinel：可能各执一词（1:1 平票）
>   - **3 个及以上**：可达成多数共识（如 2/3 同意即判定故障）
>
> 💡 Sentinel 本身**无状态**，可部署在独立机器或与 Redis 实例共存（不推荐）。

Sentinel是怎么做到确认某一节点下线/宕机的呢？实际上Sentinel节点会对其所配置的所有集群实例发送心跳包，当某实例未响应心跳包时，就会确认这一节点下线了：

**主观下线**：某一Sentinel节点发送心跳包，发现一实例未响应自己的心跳包，该节点认为此实例**主观下线**

**客观下线**：若对某一实例的**主观下线**数量达到一定数量，则认定该实例**客观下线**

倘若客观下线的是**主节点**，那么就需要从新选举一个从节点成为主节点：

**Leader 选举**

- 所有 Sentinel 通过 **Raft 算法**选举一个 Leader（负责执行故障转移）

 **故障转移**

Leader Sentinel 执行：

1. 从从节点中**选择一个最优的**作为新主（优先级：`replica-priority` → 复制偏移量 → runid 字典序）
   1. 根据`replica-priority`优先级选举新主节点，`replica-priority`在conf文件中给出，值越小优先级越高
   2. 如`replica-priority`均一致，则判断offset，offset越大说明从节点的同步数据记录更新，优先级越高
   3. offset也一致，那么就通过**从节点的runid**随便挑一个（不是主节点的runid）
2. 向选中的从节点发送 `REPLICAOF NO ONE`，使其成为主
3. 向其他从节点发送 `REPLICAOF <new-master>`，让它们复制新主
4. **更新主节点配置**，并通知所有 Sentinel 和客户端

最后会修改故障节点的配置，使之被新的主节点slave



#### 主从集群及哨兵配置

**redis.conf**:

```bash
# 此为从节点，主节点无需额外配置
# 端口（避免与主节点冲突）
port 6380

# 绑定地址
bind 0.0.0.0

# 后台运行
daemonize yes

# 日志和 PID
logfile /var/log/redis/redis-replica.log
pidfile /var/run/redis_6380.pid

# 数据目录
dir /var/lib/redis

# 关键：指定主节点 IP 和端口
replicaof 192.168.1.100 6379

# 只读模式（默认开启，推荐保持）
replica-read-only yes

# 如果主节点有密码，需配置（重要！）
masterauth your_master_password

# 如果使用 ACL（Redis 6+），可能需要 replicauser（较少用）
# replica-announce-ip 192.168.1.101   # 用于 NAT/代理场景（可选）
# replica-announce-port 6380
```



**sentinel.conf**：

```bash
# 假如需要自定义ip
# sentinel announce-ip 192.168.1.101

# 启动端口号，默认26379
port 26380

# 监控名为 mymaster 的主节点（IP + 端口）
# quorum = 2 表示至少 2 个 Sentinel 同意才判定客观下线
sentinel monitor mymaster 192.168.1.100 6379 2

# 主节点密码（如果设置了 requirepass）
sentinel auth-pass mymaster your_redis_password

# 判定主观下线时间（毫秒）
sentinel down-after-milliseconds mymaster 30000

# 故障转移超时（毫秒）
sentinel failover-timeout mymaster 180000

# 并行同步从节点数量（避免主节点压力过大）
sentinel parallel-syncs mymaster 1

# 通知脚本（可选）
sentinel notification-script mymaster /path/to/notify.sh
```

> ⚠️ **每个 Sentinel 必须配置相同的 `mymaster` 名称**，才能协同工作.

完成sentinel的配置后，我们还需要去执行命令以启动sentinel服务：

```bash
redis-sentinel path/to/sentinel.conf
```



**SpringBoot中的Redis配置：**

依赖只需要引入自带的data-redis即可，其内部包含了lettuce：

```xml
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-redis</artifactId>
    </dependency>
```

> `spring-boot-starter-data-redis` 内部默认依赖 **Lettuce**（非 Jedis）

```yml
# application.yml
spring:
  redis:
    sentinel:
      master: mymaster               # Sentinel 中定义的主节点名称，注意这里只用连接sentinel就行，Redis不配置
      nodes:
        - 192.168.1.10:26379		 # sentinel的ip以及端口，通过sentinel发现Redis
        - 192.168.1.11:26379
        - 192.168.1.12:26379
    password: your_password          # 如果有密码
    lettuce:
      pool:
        max-active: 8
        max-idle: 4
        min-idle: 1
    # 👇 关键：开启读写分离
    read-from: REPLICA_PREFERRED     # 优先从从节点读
```

| 策略                    | 说明                                         | 适用场景                                     |
| :---------------------- | :------------------------------------------- | :------------------------------------------- |
| **`MASTER`**            | 所有读写都走**主节点**                       | - 默认策略 - 强一致性要求 - 无从节点环境     |
| **`MASTER_PREFERRED`**  | 优先读主，主不可用时**自动降级读从**         | - 兼顾一致性与可用性 - 主节点通常负载不高    |
| **`REPLICA`**           | **强制只读从节点**，主节点不处理任何读请求   | - 主节点压力极大 - 业务容忍数据延迟          |
| **`REPLICA_PREFERRED`** | **优先读从节点**，从不可用时**回退到主节点** | ✅ **生产环境最推荐** - 提升读吞吐 - 自动容错 |

接下来就可以直接同之前的`StringRedisTemplate`一样用了。





### 分片集群

在主从集群中，我们还是没能解决两个问题：**高并发写**，**海量数据的存储**；

对于写入操作，我们还是靠单节点完成，甚至需要额外的资源开销以完成与从节点的数据同步；

对于海量数据的存储，我们是完全复制给所有节点而并非分摊压力；

所以我们还是需要在此基础之上完成更进一步的优化，也即解决数据存储压力与请求压力的缓解。Redis其实就推出了一个方案以解决这样的问题，也就是分片集群（Redis Cluster）：

#### 概念与实现原理

Redis **分片集群（Redis Cluster）** 是 Redis 官方提供的**分布式解决方案**，用于解决单机 Redis 的 **容量瓶颈（内存限制）** 和 **性能瓶颈（单节点 QPS 限制）**。它通过**数据分片（Sharding） + 高可用（主从复制 + 自动故障转移）** 实现水平扩展。

| 特性                     | 说明                                                  |
| :----------------------- | :---------------------------------------------------- |
| **数据分片（Sharding）** | 数据按 **16384 个哈希槽（Hash Slot）** 分布到多个节点 |
| **去中心化**             | 无中心节点，所有节点平等通信（Gossip 协议）           |
| **高可用**               | 每个分片支持 **1 主 N 从**，主挂后自动选举从节点接替  |
| **客户端直连**           | 客户端直接连接任意节点，**自动重定向**到正确节点      |
| **在线扩缩容**           | 支持动态添加/删除节点，**数据自动迁移**               |

主要特征为：

- 集群多主节点，不同主节点存储**不同数据**；
- 每个主节点有多个从节点；
- 主节点之间通过心跳包检测彼此节点的健康状态（替代哨兵方案的sentinel检测实例）；
- 客户端访问任意节点，最终都会被转接到正确节点；



#### 高可用方案

我们先前在sentinel中使用额外的sentinel哨兵检测集群实例是否正常运行，假如主节点没有正常运行则会推举一个sentinel来做新的主节点推举，其会从该主节点的从节点中推举一个成为新的主节点。

而在分片集群中，**多个主节点**互相之间也起到了类似**sentinel哨兵**的作用，正如我们先前提到的互相检测实例运行状态，其会不断检测批次状态，一个发现即为**主观下线**，超过一定阈值的主观发现即为**客观下线**。

当发现一个**主节点客观下线**时，就会**推举一个从节点**（类似哨兵的推举机制Raft）成为主节点，并**广播通知全部节点更新拓扑结构**，下次前端请求就会**重定向**到新的主节点。

| 问题                           | 答案                                               |
| :----------------------------- | :------------------------------------------------- |
| **Cluster 主节点挂了怎么办？** | **自动故障转移**（无需哨兵）                       |
| **需要哨兵吗？**               | ❌ **不需要**！Cluster 内置高可用                   |
| **关键前提？**                 | 每个主节点必须配 **至少 1 个从节点**               |
| **切换时间？**                 | 约 `cluster-node-timeout`（默认 15 秒） + 选举时间 |



#### 分片集群配置

**每个Redis节点的conf配置：**

```bash
# 端口（每个节点不同）
port 7000

# 绑定地址（生产环境建议指定内网 IP）
bind 0.0.0.0

# 后台运行
daemonize yes

# PID 文件（避免冲突）
pidfile /var/run/redis_7000.pid

# 日志文件
logfile /var/log/redis/redis-7000.log

# 数据目录（每个节点独立）
dir /var/lib/redis/7000

# 👇 关键：启用集群模式
cluster-enabled yes

# 集群节点配置文件（自动生成，不要手动编辑）
cluster-config-file nodes-7000.conf

# 节点超时时间（毫秒），影响故障检测速度
cluster-node-timeout 15000

# 是否开启 AOF（推荐开启，提高数据安全性）
appendonly yes
appendfilename "appendonly.aof"

# 如果主节点有密码（所有节点密码必须一致！）
requirepass your_redis_password
masterauth your_redis_password  # 从节点连接主节点时用

# 客户端输出缓冲区限制（防 OOM）
client-output-buffer-limit normal 0 0 0
client-output-buffer-limit replica 256mb 64mb 60
client-output-buffer-limit pubsub 32mb 8mb 60
```



**连接集群中的Redis**

在使用 `redis.conf` **启动** Redis Cluster 节点后，**节点只是“启动了”，但彼此尚未组成集群**。还需要通过 **`redis-cli --cluster create`** 命令手动创建集群，并分配哈希槽（hash slots），之后才能确认主从关系。

```bash
redis-cli --cluster create \
  <ip1:port1> <ip2:port2> ... \
  --cluster-replicas <N>
```

**这条命令会自动完成：**

- 向每个节点发送 `CLUSTER MEET`，让它们互相发现，因此只需要在一个节点执行此命令即可
- 分配哈希槽（16384 个）给主节点
- 指定从节点复制哪个主节点
- 更新每个节点的 `nodes-*.conf` 文件

**示例：**

```bash
redis-cli --cluster create \
  127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 \
  127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 \
  --cluster-replicas 1
```

> - `--cluster-replicas 1` 表示：**每个主节点配 1 个从节点**
> - 命令会自动：
>   1. 将前 3 个节点设为主
>   2. 将后 3 个节点设为从
>   3. 将 16384 个哈希槽平均分配给 3 个主节点

`redis-cli --cluster create` 命令会 **根据 `--cluster-replicas N` 参数，自动计算需要多少个主节点，并从你提供的节点列表中选出主节点和从节点**。

假设你提供了 **M 个节点**，并指定 `--cluster-replicas N`（即每个主节点配 N 个从节点），那么：

- **所需主节点数** = `M / (N + 1)`
- **必须满足**：`M % (N + 1) == 0`，否则报错



**SpringBoot中的配置：**

data-redis原生自带对于Cluster的支持，引入即可：

```xml
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-redis</artifactId>
    </dependency>
```

yml中对于细节的设置：

```yml
spring:
  redis:
    # ❌ 不要配置 host/port（Cluster 模式不用）
    # host: localhost
    # port: 6379

    # ✅ 配置集群节点列表（至少包含一个主节点）
    cluster:
      nodes:
        - 192.168.1.10:7000
        - 192.168.1.11:7001
        - 192.168.1.12:7002
        - 192.168.1.13:7003
        - 192.168.1.14:7004
        - 192.168.1.15:7005

      # 最大重定向次数（默认 5）
      max-redirects: 3

    # 如果 Redis 有密码
    password: your_redis_password

    # 连接池配置（需要 commons-pool2 依赖）
    lettuce:
      pool:
        max-active: 8      # 最大连接数
        max-idle: 4        # 最大空闲连接
        min-idle: 1        # 最小空闲连接
        max-wait: 2000ms   # 获取连接最大等待时间

      # 集群拓扑刷新策略（可选）
      cluster:
        # 自动刷新集群拓扑（应对节点变更）
        refresh-period: 30s

        # 读写分离策略（推荐）（先读从再读主）
        read-from: REPLICA_PREFERRED
```



#### 散列插槽

Redis **哈希槽（Hash Slot）** 是 Redis Cluster 实现**数据分片（Sharding）** 的核心机制。它将整个 key 空间划分为 **16384 个固定槽位**，每个 key 通过哈希算法映射到其中一个槽，而每个槽由一个主节点负责。

**为什么是 16384 个槽？**

这是 Redis 作者精心设计的数字，原因如下：

| 原因                  | 说明                                                         |
| :-------------------- | :----------------------------------------------------------- |
| **1. 节点通信开销小** | 每个节点需广播自己负责的槽位（bitmap）。 16384 位 = 2KB，网络传输成本低。 |
| **2. 足够细粒度**     | 即使有 1000 个节点，平均每个节点也有 16 个槽，负载较均衡。   |
| **3. 兼容性好**       | CRC16 哈希结果为 16 位（最大 65536），`65536 / 4 = 16384`，计算高效。 |

> 💡 如果设为 65536 个槽：
>
> - 每个节点状态信息需 8KB（65536/8）
> - 1000 节点集群 → 心跳包过大，Gossip 协议效率下降



**key如何映射到hash插槽中？**

 **计算公式：**

```java
slot = CRC16(key) % 16384
```

- `CRC16()`：Redis 使用的 16 位循环冗余校验算法（标准实现）
- `% 16384`：取模运算，确保结果在 `[0, 16383]` 范围内

**哈希槽与节点的关系**

**1. 初始分配**

- 创建集群时（`redis-cli --cluster create`），16384 个槽**平均分配给所有主节点**。
- 例如 3 主节点：
  - Node A：槽 0 ～ 5460（共 5461 个）
  - Node B：槽 5461 ～ 10922（共 5462 个）
  - Node C：槽 10923 ～ 16383（共 5461 个）

**2.动态迁移**

- 扩容时：从现有主节点**迁移部分槽**到新主节点
- 缩容时：将待下线节点的槽**迁移到其他主节点**

> ⚡ 迁移过程对客户端透明（通过 `ASK` 重定向临时转发请求）

这样一来我们就可以只管给key和value，而不是每次在redisClient操作都需要指定某一个主节点；key被hash后就只会导向**一个hash值**，交由**负责该hash值/插槽**的**主节点和他的从节点**负责，这个传递的过程：

当客户端尝试向**随便一个redis节点**发送请求时，就会对key作hash得到hash插槽结果：

若正好该hash插槽就是由该节点负责，则直接处理命令；

若不是该节点负责，则节点返回重定向错误，客户端（如 Lettuce）自动：

1. 更新本地集群拓扑缓存
2. 重连到 `192.168.1.10:7001`
3. 重新发送命令

当然一般客户端是会有拓扑缓存的，现代 Redis 客户端（如 **Lettuce**、**Jedis 4.x+**）在连接 Redis Cluster 时，**会主动缓存集群拓扑（Cluster Topology）**，包括：

- 每个主节点负责的 **哈希槽范围（slot ranges）**
- 所有节点的 **IP + 端口 + 角色（主/从）**
- 节点的 **唯一 ID（node id）**

> **插槽与主节点的映射实现：**
>
> **Lettuce**通过维护一个**`SlotOwnership`** 数组记录每个插槽所对应的主节点来实现映射关系的记录：
>
> ```java
> for (int slot = start; slot <= end; slot++) {
>     slotOwners[slot] = masterNode;
> }
> ```
>
> 查询时：`RedisClusterNode node = slotOwners[slot];` → **O(1)**



**那么如何保存同一类数据到指定Redis实例（主节点及其从节点）呢？**

**使用 Hash Tag（最常用、最推荐）**

Redis Cluster 提供了 **Hash Tag 机制**：

> **用 `{}` 包裹 key 的一部分，集群只对 `{}` 内的内容计算哈希槽。**

```bash
{tag}rest_of_key
```

- 只有 `tag` 部分参与 `CRC16` 计算
- `rest_of_key` 不影响槽位

**📌 示例：**

```bash
# 这三个 key 会落在同一个槽（因为 tag 都是 "user1001"）
user:{user1001}:profile
user:{user1001}:orders
user:{user1001}:settings

# 即使内容不同，只要 { } 内相同，就在同一槽
log:{order_20240501}:part1
log:{order_20240501}:part2
```

**✅ 优势：**

- **天然支持多 Key 操作**：`MGET`、`MSET`、事务、Lua 脚本
- **业务语义清晰**：按实体 ID 分组
- **客户端无需特殊处理**

> 💡 这是 Redis 官方推荐的“同节点存储”方案。



#### 集群动态伸缩

**加入新的从节点：**

```bash
redis-cli --cluster add-node \
  --cluster-slave \
  --cluster-master-id <7000的node-id> \
  192.168.1.10:7006 \
  192.168.1.10:7000
```

**如何获取** `<7000的node-id>`**？**

```bash
redis-cli -p 7000 CLUSTER NODES
# 输出示例：
# a1b2c3d4... 127.0.0.1:7000@17000 master - 0 123456 1 connected 0-5460
# ↑ node-id = a1b2c3d4...
```

> ✅ 执行后，7006 自动开始复制 7000 的数据。



**加入新的主节点：**

当我们需要向正在运行的集群中加入新的节点，如果是主节点要怎么给他**分配插槽**呢？并且分配了对应的插槽后，又要怎么**迁移对应的数据**呢？

```bash
redis-cli --cluster add-node \
  192.168.1.10:7006 \    # 新节点地址
  192.168.1.10:7000      # 集群中任意现有节点（用于发现全集群）
```

- 新节点（7006）向 7000 发送 `CLUSTER MEET`
- 7000 将全集群拓扑广播给 7006
- **所有节点互相认识**，但新节点**没有分配任何哈希槽**（角色为 `master`，但无数据）

> ✅ 此时新节点已属于集群，但处于“空闲主节点”状态。

**步骤 1：迁移哈希槽到新主节点**

```bash
redis-cli --cluster reshard 192.168.1.10:7000
```

交互式操作：

```bash
How many slots do you want to move? 4096          # 迁移 4096 个槽（16384/4）
What is the receiving node ID? <7006的node-id>    # 新主节点 ID
Please enter all the source node IDs.			  # 挑选被拆出hash插槽的节点id，推荐all使得hash结果均衡，节点负载均衡
  Type 'all' to use all the nodes as source nodes for the hash slots.
  Type 'done' once you entered all the source nodes: all
Do you want to proceed with the proposed reshard plan? yes
```

> 💡 建议：
>
> - 迁移数量 = `16384 / (原主数 + 1)`（如 3→4 主，迁 4096）
> - 源节点选 `all` 让负载更均衡

**步骤 2：验证迁移结果**

```bash
redis-cli -p 7006 CLUSTER NODES
# 应看到 7006 负责部分槽，如 "connected 12288-16383"
```

> ⚡ 迁移过程：
>
> - **数据逐步从源主节点复制到 7006**
> - 客户端通过 `ASK` 重定向临时访问迁移中的 key
> - **服务不中断！**



#### 插槽数据迁移的具体实现

那我们会想，假如按照**最新的拓扑**来，那么**hash插槽1000会被从A挪到B**，客户端直接拿到**节点B**，B怎么可能会知道**hash插槽1000所对应的key**跑哪去了呢？

而且我们只是把**hash挪到了新的节点**，我们又要怎么把其所**对应的key都挪到新的节点**呢？

事实上客户端还是会先走A，因为客户端的**插槽节点映射数组**压根没更新！（集群的所谓**插槽节点映射关系**也没有更新，只有彻底完成了迁移才会更新）

> 客户端的**插槽节点映射数组**更新时机：
>
> | 更新方式                  | 触发条件                   | 行为                                   |
> | :------------------------ | :------------------------- | :------------------------------------- |
> | **被动更新（Reactive）**  | 收到 `MOVED` 或 `ASK` 错误 | 立即更新对应 slot 的节点映射           |
> | **主动刷新（Proactive）** | 定时任务（如每 30 秒）     | 重新执行 `CLUSTER SLOTS`，全量更新映射 |
>
> 所以也不必担心多客户端情况，都会和集群同步的（但是定时任务吃资源）

**🔑 核心机制：集群拓扑更新 ≠ 立即切换所有权**

Redis Cluster 的槽迁移是 **“两阶段提交”式设计**，通过 **`MIGRATING` / `IMPORTING` 状态 + 重定向** 保证一致性。

------

**✅ 详细流程（以 GET key 为例）**

 **📌 前提：**

- 槽 1000 正在从 **A → B** 迁移
- 集群拓扑（`CLUSTER SLOTS`）**仍显示 slot 1000 属于 A**
  （⚠️ 这是关键！拓扑不会提前切换）

**步骤 1️⃣：客户端查拓扑 → 连 A**

- 客户端缓存的拓扑中，**slot 1000 仍指向 Node A**
- 所以它向 **Node A** 发送 `GET key`

**步骤 2️⃣：Node A 发现槽正在迁出**

- Node A 检查本地：key 存在 ✅
- 但槽 1000 处于 `MIGRATING` 状态
- **Node A 返回：`ASK 1000 <Node_B_IP:Port>`**

> 💡 `ASK` 是临时重定向（与 `MOVED` 不同）：
>
> - `MOVED` = 永久迁移完成
> - `ASK` = “这个 key 可能要搬去 B，你先去 B 问问”

**步骤 3️⃣：客户端重试到 Node B**

- 客户端收到 `ASK` 后：

  1. 向 **Node B** 发送 `ASKING` 命令（特殊标记）
  2. 紧接着发送 `GET key`

**步骤 4️⃣：Node B 处理** `ASKING + GET`

- Node B 收到 `ASKING`，知道这是迁移中的请求

- Node B 向 Node A 发起 `MIGRATE` 命令：

  ```bash
  MIGRATE Node_A_IP Node_A_Port "" 0 5000 KEYS key
  ```

- Node A 将 `key` 的值发给 Node B，并删除本地 key

- Node B 返回 `key` 的值给客户端

**步骤 5️⃣：后续请求直连 Node B**

- 槽 1000 **迁移完成后**（所有key均已完成迁移，手工确定），集群拓扑更新为 **归属 Node B**
- 客户端刷新拓扑（被动或主动）
- 下次访问直接连 Node B，返回正常结果

------

**✅ 关键澄清：拓扑何时更新？**

| 阶段       | 拓扑中 slot 1000 归属 | 客户端行为                   |
| :--------- | :-------------------- | :--------------------------- |
| **迁移前** | Node A                | 直连 A                       |
| **迁移中** | **仍为 Node A**       | 直连 A → 收到 `ASK` → 重试 B |
| **迁移后** | Node B                | 直连 B                       |

> 🚫 **绝不会出现“拓扑已切到 B，但 key 还在 A”的情况！**
> 因为 **拓扑切换（`SETSLOT ... NODE`）是迁移完成的最后一步**。

而且**迁移的触发**不是交由集群自己去一个个遍历每个节点的key，而是由**客户端访问该插槽**，走**插槽节点映射数组**
—>拿到当前key所在节点（key在该节点或至少有迁移节点，迁移完成才会更新拓扑即**插槽节点映射数组**）
—>连接该节点看该hash槽位（不是key，**节点不记录key和hash的映射**）是否需要迁移，若需要则客户端再发一个ask到对应的新节点
—>新节点接收到ask，根据信息向原节点请求获取对应key，原节点给出并删除该kv键值对，留一个去向元数据
—>所有key完成迁移，slot完成迁移，这时手工确认迁移已经完成，集群更新拓扑，所有客户端被动/主动更新**插槽节点映射数组**

**那假如一个slot有多个对应的key怎么办？**

| 问题                                  | 答案                                                |
| :------------------------------------ | :-------------------------------------------------- |
| **Redis 会自动迁移槽内所有 key 吗？** | ❌ **不会！** 默认只迁移被访问的 key                 |
| **如何确保全量迁移？**                | ✅ 必须手动触发： `--cluster rebalance` 或自定义脚本 |
| **不迁移冷数据会怎样？**              | ⚠️ 执行 `SETSLOT ... NODE` 后，冷数据**永久丢失**    |
| **迁移中断能恢复吗？**                | ✅ 能！槽状态持久化，支持续传                        |



#### 故障转移

当主节点**意外宕机**时，从节点会自动发起故障转移。

 **🔧 触发条件**

1. 主节点失联

   - 从节点检测到主节点 PING 超时（`cluster-node-timeout` 默认 15 秒）
   - 并且**大多数主节点也认为该主节点下线**（避免脑裂）

2. 从节点满足晋升条件

   - 数据较新（复制偏移量接近主节点）
- 优先级最高（多个从节点时）

------

 **🔄 自动故障迁移流程**

 **步骤 1️⃣：从节点标记主节点为** ***\*PFAIL（主观下线）\****

- 某个从节点（如 A1）发现主节点 A 无响应
- 在本地将 A 标记为 `PFAIL`

 **步骤 2️⃣：广播 PFAIL，达成** ***\*FAIL（客观下线）\****

- A1 向其他节点发送 `PING` 包，附带 “A is PFAIL”
- 当**超过半数主节点**同意 A 下线 → 集群将 A 标记为 `FAIL`
- 所有节点广播 `FAIL` 消息

> 📌 **为什么需要多数派？**
> 防止网络分区时多个从节点同时晋升（脑裂）

 **步骤 3️⃣：从节点发起** ***\*FAILOVER_AUTH_REQUEST\****

- A1 向所有主节点发送选举请求
- 主节点回复 `FAILOVER_AUTH_ACK`（一票一票投）

 **步骤 4️⃣：获得多数票 → 晋升为主节点**

- A1 收到 ≥ `(N/2 + 1)` 票（N = 主节点总数）
- A1 执行：
  1. 将自己角色改为 `master`
  2. 接管原主节点 A 的**所有哈希槽**
  3. 广播 `PONG` 消息，宣告新身份

 **步骤 5️⃣：客户端自动重连**

- 客户端收到 `MOVED` 重定向（指向 A1）
- 更新本地拓扑缓存
- 后续请求直连新主节点 A1

> ⚡ 整个过程通常 **10～30 秒**（取决于 `cluster-node-timeout`）

> 还可能是运维**自己关闭的主节点**：
>
> 用于**计划内维护**（如升级主节点），**不依赖主节点宕机**。
>
> **命令：**
>
> ```bash
> # 在从节点上执行
> redis-cli -p 7001 CLUSTER FAILOVER
> ```
>
> **🔄 流程特点：**
>
> 1. **主节点必须在线**
> 2. 从节点先与主节点**同步最新数据**
> 3. 然后安全切换，**零数据丢失**
> 4. 切换完成后，原主节点变为从节点
>
> ✅ 适合：滚动升级、硬件更换等场景

和哨兵的区别：

| 方面         | Sentinel                  | Cluster                                    |
| :----------- | :------------------------ | :----------------------------------------- |
| **决策者**   | Sentinel 进程（外部协调） | **从节点** + 多数主节点（去中心化）        |
| **选举方式** | Sentinel 投票             | Raft-like 投票（主节点参与）               |
| **切换速度** | 通常更快（1～10 秒）      | 稍慢（10～30 秒，默认 `node-timeout=15s`） |
| **脑裂防护** | 依赖 Sentinel 多数派      | 依赖主节点多数派                           |
